{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_unet_colab_clean.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "THvliaFGp1Y3",
        "colab_type": "code",
        "outputId": "8e49cbca-e3e2-4f5e-a592-ea2d08a288db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "# %keras_version 2.2.4\n",
        "# !pip install tf-nightly-gpu-2.0-preview"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIhlWXb6mhR1",
        "colab_type": "code",
        "outputId": "c8d71f5f-a396-4635-9a41-ae9b34c545d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os, datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "from tensorflow.python.keras.models import *\n",
        "from tensorflow.python.keras.layers import Input, merge, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Cropping2D, Add, Conv2DTranspose, BatchNormalization\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.python.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.losses import sparse_categorical_crossentropy, categorical_crossentropy\n",
        "from tensorflow.python.keras import backend as K\n",
        "from PIL import Image\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "import random\n",
        "import pdb\n",
        "import glob\n",
        "\n",
        "from tensorboardcolab import *\n",
        "# import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yzo56tgSuW8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !mount --bind /content/drive/My\\ Drive /content/MyDrive\n",
        "# %cd /content/MyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJSJj9_gi74o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install tensorboardcolab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsM71eQlU4qn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# logs_base_dir = \"/content/MyDrive/Data_ML/Glo/tb_logs\"\n",
        "# os.makedirs(logs_base_dir, exist_ok=True)\n",
        "# %load_ext tensorboard\n",
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir {logs_base_dir} #--host localhost --port 8088\n",
        "# tensorboard --logdir=data/ --host localhost --port 8088"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axobZfydVFqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logs_base_dir, histogram_freq=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dAHB4E9PuBMt",
        "colab": {}
      },
      "source": [
        "#downloaded = drive.CreateFile({'id':'https://drive.google.com/open?id=1h1pOR_x9mZS8Ayo-8zGJCxAPNTF_Poci'}) # replace the id with id of file you want to access"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mjvQTWVcyzEE",
        "colab": {}
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E7QYtUk9s-X",
        "colab_type": "code",
        "outputId": "9bb95edc-06d9-4c92-9cef-a5f5fb88c860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dTU04lzn17k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class U_net(object):\n",
        "\n",
        "\tdef __init__(self, img_rows = 1024, img_cols = 1024):\n",
        "\n",
        "\t\tself.img_rows = img_rows\n",
        "\t\tself.img_cols = img_cols\n",
        "\t\tself.current_folder_path = \"/content/drive/My Drive/Data_ML/Glo\"\n",
        "\t\tself.data_dir_path = self.current_folder_path + \"/data\"\n",
        "\t\tself.results_dir_path = self.current_folder_path + \"/results\"\n",
        "\t\tself.result_img_path = self.results_dir_path + \"/mgs_mask_test.npy\"\n",
        "\t\tself.train_dir_path = self.data_dir_path + \"/train\"\n",
        "\t\tself.img_dir_path = self.train_dir_path + \"/image\"\n",
        "\t\tself.label_dir_path = self.train_dir_path + \"/label\"\n",
        "\t\tself.img_org_dir = self.train_dir_path + \"/image_org\"\n",
        "\t\tself.img_play_dir = self.train_dir_path + \"/image_play\"\n",
        "\t\tself.label_play_dir = self.train_dir_path + \"/label_play\"\n",
        "\t\tself.label_org_dir = self.train_dir_path + \"/label_org\"\n",
        "\t\tself.test_img_play_dir = self.data_dir_path + \"/test\"\n",
        "\t\tself.npy_dir_path = self.current_folder_path + \"/npydata\"\n",
        "\n",
        "\t\tself.smooth = 1e-10\n",
        "\n",
        "\tdef categorical_focal_loss_fixed(self, y_true, y_pred):\n",
        "\t\t\"\"\"\n",
        "\t\t:param y_true: A tensor of the same shape as `y_pred`\n",
        "\t\t:param y_pred: A tensor resulting from a softmax\n",
        "\t\t:return: Output tensor.\n",
        "\t\t\"\"\"\n",
        "\t\tgamma=2. \n",
        "\t\talpha=.25\n",
        "\t\t# Scale predictions so that the class probas of each sample sum to 1\n",
        "\t\ty_pred /= keras.sum(y_pred, axis=-1, keepdims=True)\n",
        "\n",
        "\t\t# Clip the prediction value to prevent NaN's and Inf's\n",
        "\t\tepsilon = keras.epsilon()\n",
        "\t\ty_pred = keras.clip(y_pred, epsilon, 1. - epsilon)\n",
        "\n",
        "\t\t# Calculate Cross Entropy\n",
        "\t\tcross_entropy = -y_true * keras.log(y_pred)\n",
        "\n",
        "\t\t# Calculate Focal Loss\n",
        "\t\tloss = alpha * keras.pow(1 - y_pred, gamma) * cross_entropy\n",
        "\n",
        "\t\t# Compute mean loss in mini_batch\n",
        "\t\treturn keras.mean(loss, axis=1)\n",
        "\t# import keras.losses\n",
        "\t# keras.losses.custom_loss = custom_loss\n",
        "\tdef my_categorical_crossentropy(self, output, target, from_logits=False):\n",
        "\t\t\t\"\"\"Categorical crossentropy between an output tensor and a target tensor.\n",
        "\t\t\t# Arguments\n",
        "\t\t\t\t\toutput: A tensor resulting from a softmax\n",
        "\t\t\t\t\t\t\t(unless `from_logits` is True, in which\n",
        "\t\t\t\t\t\t\tcase `output` is expected to be the logits).\n",
        "\t\t\t\t\ttarget: A tensor of the same shape as `output`.\n",
        "\t\t\t\t\tfrom_logits: Boolean, whether `output` is the\n",
        "\t\t\t\t\t\t\tresult of a softmax, or is a tensor of logits.\n",
        "\t\t\t# Returns\n",
        "\t\t\t\t\tOutput tensor.\n",
        "\t\t\t\"\"\"\n",
        "\t\t\t# Note: tf.nn.softmax_cross_entropy_with_logits\n",
        "\t\t\t# expects logits, Keras expects probabilities.\n",
        "\t\t\tif not from_logits:\n",
        "\t\t\t\t\t# scale preds so that the class probas of each sample sum to 1\n",
        "\t\t\t\t\toutput /= tf.reduce_sum(output,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treduction_indices=len(output.get_shape()) - 1,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkeep_dims=True)\n",
        "\t\t\t\t\t# manual computation of crossentropy\n",
        "\t\t\t\t\tepsilon = _to_tensor(_EPSILON, output.dtype.base_dtype)\n",
        "\t\t\t\t\toutput = tf.clip_by_value(output, epsilon, 1. - epsilon)\n",
        "\t\t\t\t\treturn - tf.reduce_sum(target * tf.log(output),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treduction_indices=len(output.get_shape()) - 1)\n",
        "\t\t\telse:\n",
        "\t\t\t\t\treturn tf.nn.softmax_cross_entropy_with_logits(labels=target,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlogits=output)\n",
        "\n",
        "\tdef total_variation_loss(self, x):\n",
        "\t\tassert K.ndim(x) == 4\n",
        "\t\t# pdb.set_trace()\n",
        "\t\timg_nrows, img_ncols = x.shape[1:3]\n",
        "\n",
        "\t\tif K.image_data_format() == 'channels_first':\n",
        "\t\t\t\ta = K.square(\n",
        "\t\t\t\t\t\tx[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1])\n",
        "\t\t\t\tb = K.square(\n",
        "\t\t\t\t\t\tx[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:])\n",
        "\t\telse:\n",
        "\t\t\t\ta = K.square(\n",
        "\t\t\t\t\t\tx[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n",
        "\t\t\t\tb = K.square(\n",
        "\t\t\t\t\t\tx[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n",
        "\t\treturn K.sum(K.pow(a + b, 1.25))\n",
        "\t\n",
        "\tdef custom_loss (self, y_true, y_pred):\n",
        "\t\treturn categorical_crossentropy(y_true, y_pred) + 0.0001 * self.total_variation_loss(y_pred)\n",
        "\t\t# return 0.0001 * self.total_variation_loss(y_pred)\n",
        "\n",
        "\tdef my_unet_batch_norm(self, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS, LearnRate):\n",
        "\t\tinputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "\t\t# ipdb.set_trace()\n",
        "\t\tc1 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (inputs)\n",
        "\t\tb1 = BatchNormalization() (c1)\n",
        "\t\t# c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c1)\n",
        "\t\tp1 = MaxPooling2D((2, 2)) (b1)\n",
        "\n",
        "\t\tc2 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p1)\n",
        "\t\tb2 = BatchNormalization() (c2)\n",
        "\t\t# c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c2)\n",
        "\t\tp2 = MaxPooling2D((2, 2)) (b2)\n",
        "\n",
        "\t\tc3 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p2)\n",
        "\t\tb3 = BatchNormalization() (c3)\n",
        "\t\t# c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c3)\n",
        "\t\tp3 = MaxPooling2D((2, 2)) (b3)\n",
        "\n",
        "\t\tc4 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p3)\n",
        "\t\tb4 = BatchNormalization() (c4)\n",
        "\t\t# c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c4)\n",
        "\t\tp4 = MaxPooling2D((2, 2)) (b4)\n",
        "\n",
        "\t\tc5 = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4)\n",
        "\t\tb5 = BatchNormalization() (c5)\n",
        "\t\tp5 = MaxPooling2D(pool_size=(2, 2)) (b5)\n",
        "\t\t\n",
        "\t\tc6 = Conv2D(1024, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p5)\n",
        "\t\tdrop6 = Dropout(0.5) (c6)\n",
        "\t\t# c6 = Conv2D(512, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (drop6)\n",
        "\t\t\t\n",
        "\t\tu66 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same') (drop6)\n",
        "\t\tu66 = Add()([u66, c5])\n",
        "\t\tc66 = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u66)\n",
        "\t\t# c66 = Dropout(0.1) (c66)\n",
        "\t\t# c66 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c66)\n",
        "\n",
        "\t\tu6 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same') (c66)\n",
        "\t\tu6 = Add()([u6, c4])\n",
        "\t\tc6 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u6)\n",
        "\t\t# c6 = Dropout(0.1) (c6)\n",
        "\t\t# c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c6)\n",
        "\n",
        "\t\tu7 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c6)\n",
        "\t\tu7 = Add()([u7, c3])\n",
        "\t\tc7 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u7)\n",
        "\t\t# c7 = Dropout(0.1) (c7)\n",
        "\t\t# c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c7)\n",
        "\n",
        "\t\tu8 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c7)\n",
        "\t\tu8 = Add()([u8, c2])\n",
        "\t\tc8 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u8)\n",
        "\t\t# c8 = Dropout(0.1) (c8)\n",
        "\t\t# c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c8)\n",
        "\n",
        "\t\tu9 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c8)\n",
        "\t\tu9 = Add()([u9, c1])\n",
        "\t\tc9 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u9)\n",
        "\t\t# c9 = Dropout(0.1) (c9)\n",
        "\t\t# c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_uniform', padding='same') (c9)\n",
        "\n",
        "\t\toutputs = Conv2D(3, (1, 1), activation='softmax') (c9)\n",
        "\n",
        "\t\tmodel = Model(inputs=[inputs], outputs=[outputs])\n",
        "\t\tmodel.compile(optimizer = Adam(lr=LearnRate), loss = 'categorical_crossentropy' , metrics=['acc'])\n",
        "\n",
        "\t\t# model.compile(optimizer = Adam(lr=LearnRate), loss = self.custom_loss , metrics=['acc'])\n",
        "\n",
        "\t\treturn model\n",
        "\n",
        "\tdef train(self):\n",
        "\t\t# _, _, files = next(os.walk(self.img_dir_path))\n",
        "\t\tfile_names_path = glob.glob(self.img_org_dir+ \"/*.jpg\")\n",
        "\t\t# files = glob.glob(data_dir_path)\n",
        "\t\tfiles = []\n",
        "\t\tfor path in file_names_path:\n",
        "\t\t\tfolders_list = path.split(\"/\")\n",
        "\t\t\tfiles.append(folders_list[-1])\n",
        "\t\titems_no = len(files)\n",
        "\t\trandom.shuffle(files)\n",
        "\t\tbatch_size = 1\n",
        "\n",
        "\t\tmodel_path = self.current_folder_path + \"/my_unet_total_variation.hdf5\"\n",
        "\t\t# old_model_path = self.current_folder_path + \"/my_unet_model_no_focal_2.hdf5\"\n",
        "\t\t# model = load_model(old_model_path)\n",
        "\n",
        "\t\t# model = load_model(old_model_path, custom_objects={'categorical_focal_loss_fixed': self.categorical_focal_loss_fixed})\n",
        "\t\t# model = load_model(old_model_path, compile=False)\n",
        "\n",
        "\t\t# LearnRate = 0.0001\n",
        "\t\t# model.compile(optimizer = Adam(lr=LearnRate), loss = self.categorical_focal_loss_fixed , metrics=['acc'])\n",
        "\n",
        "\t\tmodel_checkpoint = ModelCheckpoint(model_path, monitor='loss',verbose=1, save_best_only=True)\n",
        "\t\ttrain_loops = 0\n",
        "\t\t# big_training = []\n",
        "\t\t# big_label = []\n",
        "\n",
        "\t\t# while len(files) >= batch_size:\n",
        "\t\t# \ttrain_items = files[:batch_size]\n",
        "\t\t# \tfiles = files[batch_size:]\n",
        "\t\t# \ttrain_batch = []\n",
        "\t\t# \tlabel_batch = []\n",
        "\t\t# for item_name in files:\n",
        "\t\t# \t# print(item_name)\n",
        "\t\t# \titem_path = self.img_org_dir + '/' + item_name\n",
        "\t\t# \tlabel_png_name = item_name[:-4] + '.png'\n",
        "\t\t# \tlabel_path = self.label_org_dir + '/' + label_png_name\n",
        "\t\t# \titem_arr = np.asarray(Image.open(item_path))\n",
        "\t\t# \tbig_training.append(item_arr)\n",
        "\t\t# \tlabel_arr = np.asarray(Image.open(label_path))\n",
        "\t\t# \tlabel_arr = label_arr.copy()\n",
        "\t\t# \tlabel_arr[label_arr==170] = 1\n",
        "\t\t# \tlabel_arr[label_arr==250] = 2\n",
        "\t\t# \t# pdb.set_trace()\n",
        "\t\t# \tlabel_arr = to_categorical(label_arr, num_classes=3, dtype = 'uint8')\n",
        "\t\t# \t# pdb.set_trace()\n",
        "\t\t# \tbig_label.append(label_arr)\n",
        "\n",
        "\t\t############################# UPLOADING TRAINING DATA FROM FOLDER###############################\n",
        "\t\tbig_training_path = self.data_dir_path + '/all_images.npy'\n",
        "\t\tbig_training = np.load(big_training_path)\n",
        "\t\tbig_label_path = self.data_dir_path + '/all_labels_3cls.npy'\n",
        "\t\tbig_label = np.load(big_label_path)\n",
        "\t\t#################################################################################################\n",
        "\t\t# big_training = np.asarray(big_training)\n",
        "\t\t# big_label = np.asarray(big_label)\n",
        "\t\tclass_weight_manual = [5, 1, 1]\n",
        "\t\t# pdb.set_trace()\n",
        "\n",
        "\t\t# logdir = os.path.join(logs_base_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\t\t# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\t\t# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n",
        "\t\t# from tensorboardcolab import *\n",
        "\t\t# tb = TensorBoardColab(startup_waiting_time=12)\n",
        "\n",
        "\t\tmodel.fit(big_training, big_label, batch_size=4, nb_epoch=200, class_weight = class_weight_manual, verbose=1, shuffle=True, callbacks=[(model_checkpoint)])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxb0FnNSmE2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(tf.__version__)\n",
        "# print(keras.__version__)\n",
        "# %tensorboard --logdir {logs_base_dir}\n",
        "\n",
        "# from tensorboardcolab import *\n",
        "# tb = TensorBoardColab()\n",
        "\n",
        "# from tensorboard import notebook\n",
        "# notebook.list()\n",
        "# notebook.display(port=6006, height=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFyKnbIsou7-",
        "colab_type": "code",
        "outputId": "bf3559c6-292e-4ccc-b9f7-2c015884888a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "myunet = U_net()\n",
        "# jaccard_distance = myunet.jaccard_distance()\n",
        "# jaccard_distance_loss = myunet.jaccard_distance_loss()\n",
        "model = myunet.my_unet_batch_norm(1024, 1024, 3, 0.0001)\n",
        "lista = ['']\n",
        "print (lista)\n",
        "myunet.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "['']\n",
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 2334 samples\n",
            "Epoch 1/200\n",
            "2332/2334 [============================>.] - ETA: 0s - loss: 4.7795 - acc: 0.6735\n",
            "Epoch 00001: loss improved from inf to 4.77679, saving model to /content/drive/My Drive/Data_ML/Glo/my_unet_total_variation.hdf5\n",
            "2334/2334 [==============================] - 962s 412ms/sample - loss: 4.7768 - acc: 0.6736\n",
            "Epoch 2/200\n",
            "2332/2334 [============================>.] - ETA: 0s - loss: 4.5174 - acc: 0.6803\n",
            "Epoch 00002: loss improved from 4.77679 to 4.51559, saving model to /content/drive/My Drive/Data_ML/Glo/my_unet_total_variation.hdf5\n",
            "2334/2334 [==============================] - 929s 398ms/sample - loss: 4.5156 - acc: 0.6803\n",
            "Epoch 3/200\n",
            "2332/2334 [============================>.] - ETA: 0s - loss: 3.8713 - acc: 0.7032\n",
            "Epoch 00003: loss improved from 4.51559 to 3.87207, saving model to /content/drive/My Drive/Data_ML/Glo/my_unet_total_variation.hdf5\n",
            "2334/2334 [==============================] - 932s 399ms/sample - loss: 3.8721 - acc: 0.7032\n",
            "Epoch 4/200\n",
            "2332/2334 [============================>.] - ETA: 0s - loss: 3.2312 - acc: 0.7268\n",
            "Epoch 00004: loss improved from 3.87207 to 3.23116, saving model to /content/drive/My Drive/Data_ML/Glo/my_unet_total_variation.hdf5\n",
            "2334/2334 [==============================] - 937s 401ms/sample - loss: 3.2312 - acc: 0.7268\n",
            "Epoch 5/200\n",
            "2332/2334 [============================>.] - ETA: 0s - loss: 2.8631 - acc: 0.7426\n",
            "Epoch 00005: loss improved from 3.23116 to 2.86232, saving model to /content/drive/My Drive/Data_ML/Glo/my_unet_total_variation.hdf5\n",
            "2334/2334 [==============================] - 938s 402ms/sample - loss: 2.8623 - acc: 0.7426\n",
            "Epoch 6/200\n",
            "2332/2334 [============================>.] - ETA: 0s - loss: 2.5941 - acc: 0.7576\n",
            "Epoch 00006: loss improved from 2.86232 to 2.59300, saving model to /content/drive/My Drive/Data_ML/Glo/my_unet_total_variation.hdf5\n",
            "2334/2334 [==============================] - 940s 403ms/sample - loss: 2.5930 - acc: 0.7576\n",
            "Epoch 7/200\n",
            "2332/2334 [============================>.] - ETA: 0s - loss: 2.5079 - acc: 0.7679\n",
            "Epoch 00007: loss improved from 2.59300 to 2.50821, saving model to /content/drive/My Drive/Data_ML/Glo/my_unet_total_variation.hdf5\n",
            "2334/2334 [==============================] - 939s 403ms/sample - loss: 2.5082 - acc: 0.7679\n",
            "Epoch 8/200\n",
            "2332/2334 [============================>.] - ETA: 0s - loss: 2.4228 - acc: 0.7773\n",
            "Epoch 00008: loss improved from 2.50821 to 2.42142, saving model to /content/drive/My Drive/Data_ML/Glo/my_unet_total_variation.hdf5\n",
            "2334/2334 [==============================] - 940s 403ms/sample - loss: 2.4214 - acc: 0.7774\n",
            "Epoch 9/200\n",
            "2332/2334 [============================>.] - ETA: 0s - loss: 2.3027 - acc: 0.7879\n",
            "Epoch 00009: loss improved from 2.42142 to 2.30164, saving model to /content/drive/My Drive/Data_ML/Glo/my_unet_total_variation.hdf5\n",
            "2334/2334 [==============================] - 941s 403ms/sample - loss: 2.3016 - acc: 0.7879\n",
            "Epoch 10/200\n",
            "2332/2334 [============================>.] - ETA: 0s - loss: 2.2021 - acc: 0.7981\n",
            "Epoch 00010: loss improved from 2.30164 to 2.20216, saving model to /content/drive/My Drive/Data_ML/Glo/my_unet_total_variation.hdf5\n",
            "2334/2334 [==============================] - 940s 403ms/sample - loss: 2.2022 - acc: 0.7981\n",
            "Epoch 11/200\n",
            "   8/2334 [..............................] - ETA: 15:37 - loss: 2.1933 - acc: 0.7912"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-f6f70d796df4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlista\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlista\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmyunet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-9295913c3754>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0;31m# tb = TensorBoardColab(startup_waiting_time=12)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbig_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weight_manual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XN3b4e_pg8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}